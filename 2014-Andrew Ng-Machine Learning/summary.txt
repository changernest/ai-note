week_1
机器学习：机器程序通过学习任务的相关经验，来提升执行任务的正确率。
给你讲授学习算法就好像给你一套工具，相比于提供工具，可能更重要的，是教你如何使用这些工具。
监督学习指训练集中每个样本都带有正确答案。
回归指通过已有的数据推测出一个连续的输出值。
分类指通过已有的数据推测出离散的输出值。
无监督学习数据没有正确答案，让算法从数据中找出某种结构。

单变量线性回归：h=k1+k2x, 只含有一个特征/输入变量。
建模误差：模型所预测的值与训练集中实际值之间的差距，我们的目标是找出使得建模误差的平方和最小的模型参数。
代价函数，亦平方误差函数，或平方误差代价函数，指建模误差的平方和。误差平方代价函数，对于大多数问题，特别是回归问题，是一个合理的选择。

梯度下降：随机选择一个参数的组合，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值，因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值，选择不同的初始参数组合，可能会找到不同的局部最小值。

week_2
多变量线性回归：有多个特征向量的线性回归。
梯度下降法之特征缩放：将特征数值转化为相同的量级，有利于梯度下降算法更快地收敛。一般将所有特征都缩放到-1到1之间。
梯度下降之学习率：如果学习率过小，需要迭代很多次才能收敛；如果学习率过大，可能导致越过收敛值而无法收敛。一般考虑00.1, 0.03, 0.1, 0.3, 1, 3, 10.


